#!/usr/bin/env python3
"""
í•œêµ­ ì»¤ë®¤ë‹ˆí‹° í•«í† í”½ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ (ë­í‚¹ ê¸°ë°˜)
- ëŒ€ìƒ: í´ë¦¬ì•™, ë½ë¿Œ, ë”ì¿ , ë”´ì§€ì¼ë³´
- ì£¼ê¸°: 1ì‹œê°„
- ê¸°ì¤€: ì¡°íšŒìˆ˜/ëŒ“ê¸€ìˆ˜/ì¶”ì²œìˆ˜ TOP
"""

import requests
import json
import time
import hashlib
import os
from datetime import datetime
from bs4 import BeautifulSoup
from urllib.parse import urljoin, quote

# ì„¤ì •
CONFIG = {
    "interval_minutes": 60,
    "top_n": 10,  # ê° ì‚¬ì´íŠ¸ë‹¹ TOP N
    "min_views": 1000,  # ìµœì†Œ ì¡°íšŒìˆ˜ í•„í„°
    "min_comments": 10,  # ìµœì†Œ ëŒ“ê¸€ìˆ˜ í•„í„°
    "telegram_token": os.getenv('TELEGRAM_BOT_TOKEN'),
    "telegram_chat_id": os.getenv('TELEGRAM_CHAT_ID'),
    "seen_posts_file": "/Users/roturnjarvis/.openclaw/workspace/logs/hot_topics_seen.json",
    "log_file": "/Users/roturnjarvis/.openclaw/workspace/logs/hot_topics_monitor.log",
    "trends_file": "/Users/roturnjarvis/.openclaw/workspace/logs/hot_topics_trends.json"
}

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
}

class HotTopicsMonitor:
    def __init__(self):
        self.seen_posts = self.load_seen_posts()
        self.session = requests.Session()
        self.session.headers.update(HEADERS)
        self.trends = self.load_trends()

    def load_seen_posts(self):
        try:
            with open(CONFIG['seen_posts_file'], 'r', encoding='utf-8') as f:
                return set(json.load(f))
        except:
            return set()

    def save_seen_posts(self):
        os.makedirs(os.path.dirname(CONFIG['seen_posts_file']), exist_ok=True)
        with open(CONFIG['seen_posts_file'], 'w', encoding='utf-8') as f:
            json.dump(list(self.seen_posts), f, ensure_ascii=False)

    def load_trends(self):
        try:
            with open(CONFIG['trends_file'], 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return {"hourly": {}, "daily": {}}

    def save_trends(self):
        os.makedirs(os.path.dirname(CONFIG['trends_file']), exist_ok=True)
        with open(CONFIG['trends_file'], 'w', encoding='utf-8') as f:
            json.dump(self.trends, f, ensure_ascii=False, indent=2)

    def generate_post_id(self, title, url, source):
        content = f"{source}:{title}:{url}"
        return hashlib.md5(content.encode()).hexdigest()[:16]

    def log(self, message):
        os.makedirs(os.path.dirname(CONFIG['log_file']), exist_ok=True)
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        log_msg = f"[{timestamp}] {message}"
        print(log_msg)
        with open(CONFIG['log_file'], 'a', encoding='utf-8') as f:
            f.write(log_msg + '\n')

    def send_telegram(self, message):
        if not CONFIG['telegram_token'] or not CONFIG['telegram_chat_id']:
            print(f"[ì•Œë¦¼] {message[:200]}...")
            return

        url = f"https://api.telegram.org/bot{CONFIG['telegram_token']}/sendMessage"
        payload = {
            'chat_id': CONFIG['telegram_chat_id'],
            'text': message,
            'parse_mode': 'HTML',
            'disable_web_page_preview': True
        }

        try:
            response = requests.post(url, json=payload, timeout=10)
            return response.json()
        except Exception as e:
            self.log(f"í…”ë ˆê·¸ë¨ ì‹¤íŒ¨: {e}")

    def extract_content_summary(self, text, max_length=150):
        """ê²Œì‹œë¬¼ ë‚´ìš© ìš”ì•½"""
        # HTML ì œê±°
        soup = BeautifulSoup(text, 'html.parser')
        clean_text = soup.get_text(separator=' ', strip=True)
        # ì•ë¶€ë¶„ë§Œ ì¶”ì¶œ
        summary = clean_text[:max_length].replace('\n', ' ').replace('\r', '')
        if len(clean_text) > max_length:
            summary += "..."
        return summary

    def analyze_sentiment_detailed(self, title, content=""):
        """ìƒì„¸ ê°ì„± ë¶„ì„"""
        text = f"{title} {content}".lower()

        positive_keywords = ['ì§€ì§€', 'ì‘ì›', 'í™˜í˜¸', 'ìŠ¹ë¦¬', 'ìš°ì„¸', 'í˜¸ì¬', 'ì„±ê³µ', 'ê¸ì •', 'í¬ë§', 'ê°œì„ ']
        negative_keywords = ['ë¹„íŒ', 'ë¬¸ì œ', 'ë…¼ë€', 'ì˜í˜¹', 'í”¼í•´', 'ë°˜ëŒ€', 'ì•…ì¬', 'ì‹¤íŒ¨', 'ë¶€ì •', 'ìš°ë ¤', 'ì‚¬ê³¼', 'ê·œíƒ„']
        angry_keywords = ['ë¶„ë…¸', 'ê²©ë¶„', 'í™˜ì¥', 'ë¯¸ì¹œ', 'ê°œXX', 'ì¢ŒíŒŒ', 'ìš°íŒŒ', 'ê·¹ìš°', 'ê·¹ì¢Œ']

        pos_count = sum(1 for word in positive_keywords if word in text)
        neg_count = sum(1 for word in negative_keywords if word in text)
        angry_count = sum(1 for word in angry_keywords if word in text)

        if angry_count > 0:
            return {'sentiment': 'ê²©ì•™', 'emoji': 'ğŸ”¥', 'score': -2}
        elif neg_count > pos_count:
            return {'sentiment': 'ë¶€ì •', 'emoji': 'ğŸ”´', 'score': -1}
        elif pos_count > neg_count:
            return {'sentiment': 'ê¸ì •', 'emoji': 'ğŸŸ¢', 'score': 1}
        else:
            return {'sentiment': 'ì¤‘ë¦½', 'emoji': 'âšª', 'score': 0}

    def categorize_topic(self, title):
        """í† í”½ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        categories = {
            'ì •ì¹˜': ['ì´ì¬ëª…', 'ìœ¤ì„ì—´', 'êµ­í˜', 'ë¯¼ì£¼ë‹¹', 'ëŒ€ì„ ', 'ì„ ê±°', 'êµ­íšŒ', 'ì •ë¶€', 'ëŒ€í†µë ¹', 'ì•¼ë‹¹', 'ì—¬ë‹¹'],
            'ê²½ì œ': ['ì£¼ì‹', 'ì¦ì‹œ', 'ì½”ì¸', 'ë¶€ë™ì‚°', 'ì§‘ê°’', 'ê¸ˆë¦¬', 'í™˜ìœ¨', 'ë¬¼ê°€', 'ê²½ê¸°', 'ê¸°ì—…', 'ì‚°ì—…'],
            'ì‚¬íšŒ': ['ì‚¬ê±´', 'ì‚¬ê³ ', 'ë²”ì£„', 'ë²•ì›', 'ì¬íŒ', 'ê²½ì°°', 'ì†Œë°©', 'ì¬ë‚œ', 'ì•ˆì „'],
            'ë…¸ë™': ['ë…¸ì¡°', 'íŒŒì—…', 'ìµœì €ì„ê¸ˆ', 'ê·¼ë¡œ', 'í•´ê³ ', 'ë…¸ë™ì', 'ì§ì¥'],
            'êµìœ¡': ['í•™êµ', 'ìˆ˜ëŠ¥', 'ëŒ€í•™', 'í•™ìƒ', 'êµì‚¬', 'êµìœ¡', 'ì…ì‹œ'],
            'IT/í…Œí¬': ['AI', 'ì¸ê³µì§€ëŠ¥', 'ì• í”Œ', 'êµ¬ê¸€', 'ì‚¼ì„±', 'ì¹´ì¹´ì˜¤', 'ë„¤ì´ë²„', 'ìŠ¤íƒ€íŠ¸ì—…', 'ê¸°ìˆ '],
            'êµ­ì œ': ['ë¯¸êµ­', 'ì¤‘êµ­', 'ì¼ë³¸', 'ë¶í•œ', 'ìš°í¬ë¼ì´ë‚˜', 'ì¤‘ë™', 'ì „ìŸ', 'ì™¸êµ'],
            'ë¬¸í™”': ['ì˜í™”', 'ë“œë¼ë§ˆ', 'ì—°ì˜ˆ', 'ìŒì•…', 'ì˜ˆìˆ ', 'ìŠ¤í¬ì¸ ', 'ì¶•êµ¬', 'ì•¼êµ¬']
        }

        title_lower = title.lower()
        for cat, keywords in categories.items():
            if any(kw in title_lower for kw in keywords):
                return cat
        return 'ê¸°íƒ€'

    def fetch_clien_hot(self):
        """í´ë¦¬ì•™ ì¸ê¸° ê²Œì‹œë¬¼"""
        posts = []
        try:
            # ì¶”ì²œ ë§ì€ ìˆœ
            url = "https://www.clien.net/service/board/park?&od=T33"  # ê³µê° ìˆœ
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')

            items = soup.find_all('div', class_='list_item')
            for item in items[:CONFIG['top_n']]:
                try:
                    title_elem = item.find('span', class_='subject_fixed')
                    if not title_elem:
                        continue

                    title = title_elem.get_text(strip=True)
                    link_elem = title_elem.find_parent('a')
                    url = link_elem['href'] if link_elem else ''
                    if url and not url.startswith('http'):
                        url = f"https://www.clien.net{url}"

                    # ì¡°íšŒìˆ˜, ëŒ“ê¸€, ì¶”ì²œ
                    hit_elem = item.find('span', class_='hit')
                    comment_elem = item.find('span', class_='rSymph05')
                    like_elem = item.find('span', class_='recommend')

                    views = int(hit_elem.get_text().replace(',', '')) if hit_elem else 0
                    comments = int(comment_elem.get_text()) if comment_elem else 0
                    likes = int(like_elem.get_text()) if like_elem else 0

                    if views >= CONFIG['min_views'] or comments >= CONFIG['min_comments']:
                        posts.append({
                            'source': 'í´ë¦¬ì•™',
                            'title': title,
                            'url': url,
                            'views': views,
                            'comments': comments,
                            'likes': likes,
                            'time': datetime.now().strftime('%H:%M')
                        })
                except Exception as e:
                    continue

            time.sleep(2)
        except Exception as e:
            self.log(f"í´ë¦¬ì•™ ì˜¤ë¥˜: {e}")

        return posts

    def fetch_ppomppu_hot(self):
        """ë½ë¿Œ ì¸ê¸° ê²Œì‹œë¬¼"""
        posts = []
        try:
            # ì¸ê¸°ê¸€ ì¡°íšŒìˆœ ì •ë ¬
            url = "https://www.ppomppu.co.kr/zboard/zboard.php?id=freeboard&sort=read_num&how=desc"
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ì‹¤ì œ ê²Œì‹œë¬¼ tr ì°¾ê¸° (ì¡°íšŒìˆ˜ê°€ 1000 ì´ìƒì¸)
            all_tr = soup.find_all('tr')
            
            for tr in all_tr:
                try:
                    tds = tr.find_all('td')
                    if len(tds) < 5:
                        continue
                    
                    # ë§ˆì§€ë§‰ tdê°€ ì¡°íšŒìˆ˜ì¸ì§€ í™•ì¸
                    views_text = tds[-1].get_text(strip=True).replace(',', '')
                    if not views_text.isdigit():
                        continue
                    
                    views = int(views_text)
                    
                    # ì¡°íšŒìˆ˜ 500 ì´ìƒë§Œ
                    if views < 500:
                        continue
                    
                    # ì œëª© ì°¾ê¸° (ë³´í†µ td[1])
                    title_td = None
                    for td in tds[1:4]:
                        link = td.find('a', href=True)
                        if link:
                            title_text = link.get_text(strip=True)
                            if len(title_text) > 5 and 'javascript' not in link.get('href', ''):
                                title_td = td
                                break
                    
                    if not title_td:
                        continue
                    
                    link = title_td.find('a', href=True)
                    title = link.get_text(strip=True)
                    href = link.get('href', '')
                    
                    # ëŒ“ê¸€ ìˆ˜ ì°¾ê¸° (ì œëª©ì— [n] í˜•íƒœ)
                    comments = 0
                    import re
                    comment_match = re.search(r'\[(\d+)\]$', title)
                    if comment_match:
                        comments = int(comment_match.group(1))
                        title = re.sub(r'\[\d+\]$', '', title).strip()
                    
                    # ê³µì§€/ê·œì¹™ ì œì™¸
                    skip_keywords = ['ê·œì¹™', 'ê³µì§€', 'í•„ë…', 'ì´ìš©ì•ˆë‚´', 'ìš´ì˜ì›ì¹™']
                    if any(kw in title for kw in skip_keywords):
                        continue
                    
                    # ì‘ì„±ì (td[2] ë˜ëŠ” td[3])
                    author = ""
                    for td in tds[2:4]:
                        text = td.get_text(strip=True)
                        if text and not text.isdigit():
                            author = text
                            break
                    
                    full_url = href if href.startswith('http') else f"https://www.ppomppu.co.kr/zboard/{href}"
                    
                    posts.append({
                        'source': 'ë½ë¿Œ',
                        'title': title,
                        'url': full_url,
                        'views': views,
                        'comments': comments,
                        'likes': 0,
                        'time': datetime.now().strftime('%H:%M')
                    })
                    
                    if len(posts) >= CONFIG['top_n']:
                        break
                        
                except Exception as e:
                    continue
            
            time.sleep(2)
        except Exception as e:
            self.log(f"ë½ë¿Œ ì˜¤ë¥˜: {e}")
        
        return posts

    def fetch_theqoo_hot(self):
        """ë”ì¿  ì¸ê¸° ê²Œì‹œë¬¼"""
        posts = []
        try:
            url = "https://theqoo.net/hot"
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # í…Œì´ë¸”ì—ì„œ tr ì¶”ì¶œ
            tables = soup.find_all('table', {'class': lambda x: x})
            if not tables:
                return posts
            
            all_tr = tables[0].find_all('tr')
            
            for tr in all_tr:
                try:
                    tds = tr.find_all('td')
                    if len(tds) < 5:
                        continue
                    
                    # td[0]: ë²ˆí˜¸, td[1]: ì¹´í…Œê³ ë¦¬, td[2]: ì œëª©, td[3]: ì‹œê°„, td[4]: ì¡°íšŒìˆ˜
                    # ë²ˆí˜¸ê°€ ìˆ«ìì¸ì§€ í™•ì¸ (ê³µì§€ ì œì™¸)
                    no_text = tds[0].get_text(strip=True)
                    if not no_text.isdigit():
                        continue
                    
                    # ì œëª© tdì—ì„œ ë§í¬ ì¶”ì¶œ
                    title_td = tds[2]
                    link = title_td.find('a', href=True)
                    if not link:
                        continue
                    
                    title = link.get_text(strip=True)
                    href = link.get('href', '')
                    
                    # ì¹´í…Œê³ ë¦¬
                    category = tds[1].get_text(strip=True)
                    
                    # ì¡°íšŒìˆ˜
                    views_text = tds[4].get_text(strip=True).replace(',', '')
                    views = int(views_text) if views_text.isdigit() else 0
                    
                    if views < 500:
                        continue
                    
                    # ëŒ“ê¸€ ìˆ˜ (ì œëª©ì— [n] í˜•íƒœ)
                    comments = 0
                    import re
                    comment_match = re.search(r'\[(\d+)\]$', title)
                    if comment_match:
                        comments = int(comment_match.group(1))
                        title = re.sub(r'\[\d+\]$', '', title).strip()
                    
                    posts.append({
                        'source': 'ë”ì¿ ',
                        'title': title,
                        'url': href if href.startswith('http') else f"https://theqoo.net{href}",
                        'views': views,
                        'comments': comments,
                        'likes': 0,
                        'category_tag': category,
                        'time': datetime.now().strftime('%H:%M')
                    })
                    
                    if len(posts) >= CONFIG['top_n']:
                        break
                        
                except Exception as e:
                    continue
            
            time.sleep(2)
        except Exception as e:
            self.log(f"ë”ì¿  ì˜¤ë¥˜: {e}")
        
        return posts

    def fetch_ddanzi_hot(self):
        """ë”´ì§€ì¼ë³´ ì¸ê¸° ê²Œì‹œë¬¼"""
        posts = []
        try:
            url = "https://www.ddanzi.com/free?sort_index=readed_count"
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ë”´ì§€ëŠ” /free/NUMBER í˜•íƒœì˜ ë§í¬
            import re
            all_links = soup.find_all('a', href=True)
            seen_titles = set()
            
            for link in all_links:
                try:
                    href = link.get('href', '')
                    title = link.get_text(strip=True)
                    
                    # íŒ¨í„´: /free/ìˆ«ì
                    if not re.match(r'.*/free/\d+$', href):
                        continue
                    
                    # ì œëª© ê¸¸ì´ ì²´í¬
                    if len(title) < 10 or len(title) > 200:
                        continue
                    
                    # ì¤‘ë³µ ì œê±°
                    if title in seen_titles:
                        continue
                    seen_titles.add(title)
                    
                    # ë¶€ëª¨ ìš”ì†Œì—ì„œ ë©”íƒ€ ì •ë³´ ì°¾ê¸°
                    parent = link.find_parent(['li', 'div', 'tr'])
                    views = 0
                    comments = 0
                    
                    if parent:
                        parent_text = parent.get_text()
                        # ì¡°íšŒìˆ˜: ìˆ«ì,ìˆ«ìíšŒ
                        view_match = re.search(r'([\d,]+)\s*íšŒ', parent_text)
                        if view_match:
                            views = int(view_match.group(1).replace(',', ''))
                    
                    # ëŒ“ê¸€: ì œëª©ì—ì„œ [n] ì¶”ì¶œ
                    comment_match = re.search(r'\[(\d+)\]', title)
                    if comment_match:
                        comments = int(comment_match.group(1))
                        title = re.sub(r'\[\d+\]', '', title).strip()
                    
                    # URL ì²˜ë¦¬
                    full_url = href if href.startswith('http') else f"https://www.ddanzi.com{href}"
                    
                    posts.append({
                        'source': 'ë”´ì§€',
                        'title': title,
                        'url': full_url,
                        'views': views,
                        'comments': comments,
                        'likes': 0,
                        'time': datetime.now().strftime('%H:%M')
                    })
                    
                    if len(posts) >= CONFIG['top_n']:
                        break
                        
                except Exception as e:
                    continue
            
            time.sleep(2)
        except Exception as e:
            self.log(f"ë”´ì§€ ì˜¤ë¥˜: {e}")
        
        return posts

    def run(self):
        """ë©”ì¸ ì‹¤í–‰"""
        self.log("í•«í† í”½ ëª¨ë‹ˆí„°ë§ ì‹œì‘")

        all_posts = []
        all_posts.extend(self.fetch_clien_hot())
        all_posts.extend(self.fetch_ppomppu_hot())
        all_posts.extend(self.fetch_theqoo_hot())
        all_posts.extend(self.fetch_ddanzi_hot())

        # ì¸ê¸°ë„ ì ìˆ˜ ê³„ì‚° (ì¡°íšŒìˆ˜ + ëŒ“ê¸€ìˆ˜*10)
        for post in all_posts:
            post['score'] = post['views'] + (post['comments'] * 10)
            post['sentiment'] = self.analyze_sentiment_detailed(post['title'])
            post['category'] = self.categorize_topic(post['title'])
            post['post_id'] = self.generate_post_id(post['title'], post['url'], post['source'])

        # ì ìˆ˜ìˆœ ì •ë ¬
        all_posts.sort(key=lambda x: x['score'], reverse=True)

        # ì¤‘ë³µ ì œê±° ë° ì‹ ê·œ í¬ìŠ¤íŠ¸ í•„í„°ë§
        new_posts = []
        for post in all_posts:
            if post['post_id'] not in self.seen_posts:
                self.seen_posts.add(post['post_id'])
                new_posts.append(post)

        self.save_seen_posts()

        # íŠ¸ë Œë“œ ì—…ë°ì´íŠ¸
        hour = datetime.now().strftime('%H:00')
        if hour not in self.trends['hourly']:
            self.trends['hourly'][hour] = []
        self.trends['hourly'][hour].extend([p['category'] for p in new_posts])
        self.save_trends()

        # ê²°ê³¼ ì¶œë ¥ ë° ì•Œë¦¼
        if new_posts:
            self.log(f"ì‹ ê·œ í•«í† í”½ {len(new_posts)}ê°œ ë°œê²¬")
            self.send_notification(new_posts)
        else:
            self.log("ì‹ ê·œ í•«í† í”½ ì—†ìŒ")

        return new_posts

    def send_notification(self, posts):
        """ì•Œë¦¼ ë©”ì‹œì§€ ìƒì„± ë° ì „ì†¡ - ì»¤ë®¤ë³„ TOP 5"""
        # ì¶œì²˜ë³„ ê·¸ë£¹í™”
        by_source = {'í´ë¦¬ì•™': [], 'ë½ë¿Œ': [], 'ë”ì¿ ': [], 'ë”´ì§€': []}
        for post in posts:
            source = post['source']
            if source in by_source:
                by_source[source].append(post)
        
        # ë©”ì‹œì§€ ìƒì„± (HTML í˜•ì‹ìœ¼ë¡œ í´ë¦­ ê°€ëŠ¥í•œ ë§í¬)
        message = f"ğŸ”¥ <b>ì‹¤ì‹œê°„ í•«í† í”½</b>\n"
        message += f"â° {datetime.now().strftime('%H:%M')} ê¸°ì¤€\n\n"
        
        # ì»¤ë®¤ë³„ TOP 5
        for source, source_posts in by_source.items():
            if not source_posts:
                continue
            
            # ì ìˆ˜ìˆœ ì •ë ¬ í›„ TOP 5
            source_posts.sort(key=lambda x: x.get('score', x['views']), reverse=True)
            top_posts = source_posts[:5]
            
            message += f"<b>ğŸ“Œ {source} TOP {len(top_posts)}</b>\n"
            
            for i, post in enumerate(top_posts, 1):
                emoji = post['sentiment']['emoji']
                title = post['title'][:30] + "..." if len(post['title']) > 30 else post['title']
                views = f"{post['views']:,}" if post['views'] > 0 else "N/A"
                comments = f"ğŸ’¬{post['comments']}" if post['comments'] > 0 else ""
                
                # í´ë¦­ ê°€ëŠ¥í•œ ë§í¬ (HTML)
                message += f"{i}. {emoji} <a href='{post['url']}'>{title}</a>\n"
                message += f"   ğŸ‘ {views} {comments}\n"
            
            message += "\n"
        
        # ì „ì²´ í†µê³„
        total_posts = sum(len(v) for v in by_source.values())
        total_views = sum(sum(p['views'] for p in v) for v in by_source.values())
        
        message += f"<b>ğŸ“Š ì „ì²´:</b> {total_posts}ê°œ ê²Œì‹œë¬¼, ì´ {total_views:,} ì¡°íšŒ\n"
        message += f"<i>1ì‹œê°„ë§ˆë‹¤ ì—…ë°ì´íŠ¸</i>"
        
        print("\n" + "="*70)
        print(message.replace('<b>', '').replace('</b>', '').replace('<a href=\'', '[').replace('\'>', '] ').replace('</a>', '').replace('<i>', '').replace('</i>', ''))
        print("="*70)
        
        self.send_telegram(message)

if __name__ == "__main__":
    monitor = HotTopicsMonitor()
    monitor.run()
